{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJOblvfKZO3bO411iaBvhj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yesudoss/ResumeParser/blob/main/Resume.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co6FXwY7KQg2",
        "outputId": "36d59d9f-2f54-48e4-cb0f-a07efbd832c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (36.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "---------------Phone-----------------\n",
            "9786047451\n",
            "--------------Email------------------\n",
            "yesudoss1999@gmail.com\n",
            "To\n",
            "be\n",
            "a\n",
            "part\n",
            "of\n",
            "an\n",
            "organization\n",
            "where\n",
            "I\n",
            "get\n",
            "a\n",
            "chance\n",
            "to\n",
            "use\n",
            "my\n",
            "knowledge\n",
            "and\n",
            "skills\n",
            "to\n",
            "contribute\n",
            "to\n",
            "the\n",
            "progress\n",
            "of\n",
            "the\n",
            "organization\n",
            "as\n",
            "well\n",
            "as\n",
            "myself.\n",
            "Yesu\n",
            "Doss\n",
            "X\n",
            "Matha\n",
            "Kovil\n",
            "Street,\n",
            "Michael\n",
            "Puram\n",
            "(Village\n",
            "&\n",
            "Post),\n",
            "Sankarapuram-\n",
            "TK,\n",
            "Kallakurichi-\n",
            "DT,\n",
            "Tamil\n",
            "Nadu-605\n",
            "702,\n",
            "9786047451,\n",
            "yesudoss1999@gmail.com\n",
            "OBJECTIVE\n",
            "STRENGTHS\n",
            "✔\n",
            "Adaptive\n",
            "and\n",
            "a\n",
            "good\n",
            "team\n",
            "player\n",
            "✔\n",
            "Very\n",
            "sincere\n",
            "and\n",
            "very\n",
            "honest\n",
            "✔\n",
            "Quick\n",
            "Learner\n",
            "with\n",
            "good\n",
            "grasping\n",
            "ability\n",
            "✔\n",
            "Lead\n",
            "and\n",
            "work\n",
            "as\n",
            "a\n",
            "team\n",
            "in\n",
            "an\n",
            "organized\n",
            "way\n",
            "✔\n",
            "Active\n",
            "Learner\n",
            "TECHNICAL\n",
            "SKILLS\n",
            "Programming\n",
            "Language:\n",
            "C,\n",
            "C++,\n",
            "and\n",
            "Python\n",
            "Web\n",
            "Technology:\n",
            "HTML\n",
            "and\n",
            "CSS,\n",
            "Odoo\n",
            "Database:\n",
            "MySQL,\n",
            "PostgreSQL\n",
            "Operating\n",
            "System:\n",
            "Windows\n",
            "family,\n",
            "Ubuntu\n",
            "EDUCATION\n",
            "Don\n",
            "Bosco\n",
            "College,\n",
            "Yelagiri\n",
            "Hills\n",
            "-\n",
            "B.sc\n",
            "Computer\n",
            "Science\n",
            "JUNE\n",
            "2017\n",
            "-\n",
            "May\n",
            "2020\n",
            "with\n",
            "7.772\n",
            "CGPA\n",
            "Mount\n",
            "Carmel\n",
            "Matric\n",
            "Hr.\n",
            "Sec.\n",
            "School,\n",
            "Kallakurichi-\n",
            "10+2\n",
            "JULY\n",
            "2016\n",
            "-\n",
            "April\n",
            "2017\n",
            "with\n",
            "91.41%.\n",
            "St,\n",
            "Michael’s\n",
            "High\n",
            "School,\n",
            "Michael\n",
            "Puram\n",
            "-\n",
            "SSLC\n",
            "JUNE\n",
            "2014\n",
            "-\n",
            "April\n",
            "2015\n",
            "with\n",
            "90.40.\n",
            "ACADEMIC\n",
            "ACHIEVEMENTS\n",
            "✔\n",
            "Stood\n",
            "3rd\n",
            "in\n",
            "2nd\n",
            "Semester.\n",
            "✔\n",
            "Stood\n",
            "1st\n",
            "in\n",
            "3rd\n",
            "Semester.\n",
            "✔\n",
            "Stood\n",
            "3rd\n",
            "in\n",
            "5th\n",
            "Semester.\n",
            "✔\n",
            "Stood\n",
            "1st\n",
            "and\n",
            "2nd\n",
            "in\n",
            "debugging\n",
            "at\n",
            "state\n",
            "level\n",
            "symposiums\n",
            "✔\n",
            "Participated\n",
            "in\n",
            "the\n",
            "state\n",
            "Level\n",
            "symposium\n",
            "and\n",
            "won\n",
            "many\n",
            "prizes.\n",
            "✔\n",
            "Participated\n",
            "in\n",
            "National\n",
            "Level\n",
            "Conference\n",
            "in\n",
            "Computing\n",
            "Paradigms\n",
            "✔\n",
            "Participated\n",
            "in\n",
            "One\n",
            "day\n",
            "Workshop\n",
            "On\n",
            "IORT.\n",
            "✔\n",
            "Organized\n",
            "state\n",
            "level\n",
            "symposium\n",
            "called\n",
            "“BOSCON’19”\n",
            "in\n",
            "our\n",
            "college\n",
            "✔\n",
            "Served\n",
            "as\n",
            "a\n",
            "Student\n",
            "Secretary\n",
            "in\n",
            "College\n",
            "for\n",
            "one\n",
            "year.\n",
            "CERTIFICATE\n",
            "COURSES\n",
            "✔\n",
            "TCS\n",
            "ion\n",
            "Soft\n",
            "Skills\n",
            "✔\n",
            "Professional\n",
            "Education\n",
            "(Python)\n",
            "✔\n",
            "Tech\n",
            "Talk\n",
            "(Python\n",
            "-\n",
            "Odoo\n",
            "development)\n",
            "PERSONAL\n",
            "VITAE\n",
            "Name\n",
            "Yesu\n",
            "Doss\n",
            "X\n",
            "Father’s\n",
            "Name\n",
            "Xavier\n",
            "S\n",
            "Mother’s\n",
            "Name\n",
            "John\n",
            "Theras\n",
            "Irudaya\n",
            "Mary\n",
            "A\n",
            "Date\n",
            "of\n",
            "Birth\n",
            "11/12/1999\n",
            "Gender\n",
            "Nationality\n",
            "Male\n",
            "Indian\n",
            "Marital\n",
            "Status\n",
            "Unmarried\n",
            "Languages\n",
            "Known\n",
            "Tamil\n",
            "and\n",
            "English\n",
            "Reference\n",
            "hod-cs@dbcyelagiri.edu.in\n",
            ":\n",
            ":\n",
            ":\n",
            ":\n",
            ":\n",
            ":\n",
            ":\n",
            ":\n",
            ":\n",
            "I\n",
            "hereby\n",
            "acknowledge\n",
            "that\n",
            "all\n",
            "the\n",
            "above\n",
            "information\n",
            "is\n",
            "given\n",
            "with\n",
            "my\n",
            "Yours\n",
            "faithfully,\n",
            "YESU\n",
            "DOSS\n",
            "X\n",
            "DECLARATION\n",
            "knowledge.\n",
            "Date:\n",
            "March\n",
            "01\n",
            "2021\n",
            "Place:\n",
            "Yelagiri\n",
            "Hills\n",
            "--------------Education------------------\n",
            "[('X', '2021')]\n",
            "---------------Skills-----------------\n",
            "['C++', 'Ubuntu', 'Windows', 'Postgresql', 'Python', 'C', 'Programming', 'Css', 'Mysql', 'Debugging', 'Technical skills', 'Html', 'English', 'Technical', 'Database', 'System']\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"RESUME_PARSER_1.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1W35NyfBzqtICtE-vmnS7drsplGb0KZfW\n",
        "\"\"\"\n",
        "\n",
        "!pip install pdfminer.six\n",
        "# !pip install nltk\n",
        "\n",
        "# !pip install pandas\n",
        "\n",
        "# !python -m nltk.downloader stopwords\n",
        "# !python -m nltk nltk.download\n",
        "\n",
        "import nltk\n",
        " \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "# Added this line\n",
        "nltk.download('words')\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "from pdfminer.high_level import extract_text\n",
        "# import pdfminer\n",
        " \n",
        " \n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "def extract_names(text):\n",
        "    person_names = []\n",
        " \n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                person_names.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        " \n",
        "    return person_names\n",
        "\n",
        "def extract_mobile_number(text):\n",
        "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
        "    \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        if len(number) > 10:\n",
        "            return '+' + number\n",
        "        else:\n",
        "            return number\n",
        "def extract_email(email):\n",
        "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# load pre-trained model\n",
        "# python -m spacy download en_core_web_sm\n",
        "spacy.load('en_core_web_sm')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Grad all general stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "# Education Degrees\n",
        "EDUCATION = [\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII',\n",
        "            'B.sc', 'School'\n",
        "        ]\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # Sentence Tokenizer\n",
        "\n",
        "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
        "\n",
        "    edu = {}\n",
        "    # Extract education degree\n",
        "    for index, text in enumerate(nlp_text):\n",
        "        for tex in text.split():\n",
        "            print(tex)\n",
        "            # Replace all special symbols\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
        "                edu[tex] = text + nlp_text[index + 1]\n",
        "\n",
        "    # Extract year\n",
        "    education = []\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "        if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_skills(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    noun_chunks=nlp_text.noun_chunks\n",
        "    # removing stop words and implementing word tokenization\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    # reading the csv file\n",
        "    data = pd.read_csv(\"/content/sample_data/Yesu/skills.csv\") \n",
        "    \n",
        "    # extract values\n",
        "    skills = list(data.columns.values)\n",
        "    \n",
        "    skillset = []\n",
        "    \n",
        "    # check for one-grams (example: python)\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    # check for bi-grams and tri-grams (example: machine learning)\n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  text=extract_text_from_pdf('/content/sample_data/Yesu/MyResume.pdf')\n",
        "  names = extract_names(text)\n",
        "\n",
        "#   if names:\n",
        "#     print(names[0])\n",
        "  phone_number = extract_mobile_number(text)\n",
        "  print(\"---------------Phone-----------------\")\n",
        "  print(phone_number)\n",
        "  email_id=extract_email(text)\n",
        "  print(\"--------------Email------------------\")\n",
        "  print(email_id)\n",
        "  education=extract_education(text)\n",
        "  print(\"--------------Education------------------\")\n",
        "  print(education)\n",
        "  skills=extract_skills(text)\n",
        "  print(\"---------------Skills-----------------\")\n",
        "  print(skills)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "LXG2SUipKxI_"
      }
    }
  ]
}